{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 17.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "7e6c16c0dfdbc77bfcbe204e85d0c16584c95201480158d7c0f9a5414368d8c5"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDTohOhm5Hhs"
      },
      "source": [
        "# Fruit Classification using CNN\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u-DMUrvBoMI"
      },
      "source": [
        "If you changed something in your drive after you did the drive.mount('/content/drive'), you have to remount it with drive.mount('/content/drive', force_remount=True). force_remount is used only when you have to mount the drive irrespective of whether its loaded previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z74FEWGHejo"
      },
      "source": [
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DRhK8JPHi5i"
      },
      "source": [
        "trainingDataset = []\n",
        "classNumber = 0\n",
        "img_size = 100\n",
        "path = \"Face_Mask/Train\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6mM0bZnIbC8"
      },
      "source": [
        "The clear() method removes all items from the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qXowcS4IPDx",
        "outputId": "72fab37d-f5bd-464e-b59c-b04b0ab00f20"
      },
      "source": [
        "trainingDataset.clear()\n",
        "for folder in (os.listdir(path)):\n",
        "  print(classNumber)\n",
        "  fp = os.path.join(path,folder)\n",
        "  for eachImage in os.listdir(fp):\n",
        "    imagePath = os.path.join(fp,eachImage)\n",
        "    img = (cv2.imread(imagePath,cv2.IMREAD_GRAYSCALE))/255\n",
        "    resized = cv2.resize(img , (img_size , img_size))\n",
        "    trainingDataset.append([resized,classNumber])\n",
        "    \n",
        "  classNumber = classNumber + 1\n",
        "\n",
        "print(len(trainingDataset))\n",
        "print(len(trainingDataset[0]))\n",
        "print(trainingDataset[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "1315\n",
            "2\n",
            "[array([[0.8       , 0.80313725, 0.79607843, ..., 0.6745098 , 0.67058824,\n",
            "        0.67058824],\n",
            "       [0.78941176, 0.79333333, 0.79607843, ..., 0.6745098 , 0.67058824,\n",
            "        0.67058824],\n",
            "       [0.79215686, 0.79607843, 0.79607843, ..., 0.6745098 , 0.67058824,\n",
            "        0.67058824],\n",
            "       ...,\n",
            "       [0.74901961, 0.74901961, 0.74901961, ..., 0.51960784, 0.51568627,\n",
            "        0.48235294],\n",
            "       [0.75294118, 0.75294118, 0.75294118, ..., 0.50588235, 0.49294118,\n",
            "        0.49294118],\n",
            "       [0.75294118, 0.75294118, 0.75294118, ..., 0.49764706, 0.48235294,\n",
            "        0.48235294]]), 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvAwu4SLLKmC"
      },
      "source": [
        "Module 'time' is used to handle time-related tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP0SOCTHKp9h"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\r\n",
        "from tensorflow.keras.callbacks import TensorBoard\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "import pickle\r\n",
        "import time"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqt89nsSIYYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a390462-879b-41a0-d187-d15a087e7ef5"
      },
      "source": [
        "X = []\r\n",
        "Y = []\r\n",
        "img_size = 100\r\n",
        "np.random.shuffle(trainingDataset)\r\n",
        "for features, label in trainingDataset:\r\n",
        "    X.append(features)\r\n",
        "    Y.append(label)\r\n",
        "print(Y)    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjfvytzllzeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4baad4d-ec9d-4705-cce5-c1178df0c621"
      },
      "source": [
        "X = np.array(X).reshape(-1, img_size, img_size, 1)\n",
        "Y_binary = to_categorical(Y)\n",
        "print(Y_binary)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1.]\n [1. 0.]\n [1. 0.]\n ...\n [0. 1.]\n [1. 0.]\n [0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYHOsbtJmsII"
      },
      "source": [
        "**Overfitting:** Good performance on the training data, poor generliazation to other data. <br>\r\n",
        "**Underfitting:** Poor performance on the training data and poor generalization to other data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkkP2GTKm8Ql"
      },
      "source": [
        "**Dropout** is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.<br>\r\n",
        "(Dropout(0.3) means setting 30% inputs to zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnUexJx_mFTp"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(40, (3, 3), input_shape=(100,100,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(60, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(80, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('relu'))\n",
        " \n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# model = Sequential()\n",
        "\n",
        "# model.add(Conv2D(200, (3, 3), input_shape=(100,100,1)))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# model.add(Conv2D(100, (3, 3)))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "# model.add(Flatten())\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(50))\n",
        "# model.add(Activation('relu'))\n",
        " \n",
        "# model.add(Dense(2))\n",
        "# model.add(Activation('softmax'))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrtfZC17oEgD"
      },
      "source": [
        "TensorBoard is a tool/dashboard for providing the measurements and visualizations needed during the machine learning workflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbnpxmYqnqHU"
      },
      "source": [
        "# NAME = \"3-conv-128-layer-dense-1-out-2-softmax-categorical-cross-2-CNN\"\r\n",
        "# tensorboard = TensorBoard(log_dir=\"/content/drive/My Drive/fruitsData/Logs/{}\".format(NAME))\r\n",
        "\r\n",
        "model.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer='adam',\r\n",
        "              metrics=['accuracy'],\r\n",
        "              )"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEVCAh1Vu5E6"
      },
      "source": [
        "---- batch_size = 32 <br>\r\n",
        "---- validation_split = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKTEcxuToS2x",
        "outputId": "3676c738-fb37-4186-fe6d-d214535b7c93"
      },
      "source": [
        "# model.fit(X, Y_binary,\n",
        "#           batch_size = 32,\n",
        "#           epochs=10, validation_split = 0.3)\n",
        " \n",
        "# model.save(\"Face_Mask/face_detect/Models/{NAME}.model\")\n",
        "\n",
        "\n",
        "model.fit(X, Y_binary, batch_size = 32, epochs=20, validation_split = 0.3)\n",
        "model.save(\"face_detect/Models/{NAME}.model\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "29/29 [==============================] - 30s 930ms/step - loss: 0.8251 - accuracy: 0.5563 - val_loss: 0.6802 - val_accuracy: 0.6861\n",
            "Epoch 2/20\n",
            "29/29 [==============================] - 20s 684ms/step - loss: 0.6311 - accuracy: 0.6812 - val_loss: 0.5884 - val_accuracy: 0.7013\n",
            "Epoch 3/20\n",
            "29/29 [==============================] - 20s 680ms/step - loss: 0.5393 - accuracy: 0.7199 - val_loss: 0.5741 - val_accuracy: 0.7646\n",
            "Epoch 4/20\n",
            "29/29 [==============================] - 20s 682ms/step - loss: 0.4406 - accuracy: 0.8198 - val_loss: 0.5354 - val_accuracy: 0.7139\n",
            "Epoch 5/20\n",
            "29/29 [==============================] - 20s 686ms/step - loss: 0.3260 - accuracy: 0.8619 - val_loss: 0.3994 - val_accuracy: 0.7949\n",
            "Epoch 6/20\n",
            "29/29 [==============================] - 19s 662ms/step - loss: 0.2539 - accuracy: 0.8998 - val_loss: 0.2592 - val_accuracy: 0.9089\n",
            "Epoch 7/20\n",
            "29/29 [==============================] - 20s 674ms/step - loss: 0.1915 - accuracy: 0.9321 - val_loss: 0.4683 - val_accuracy: 0.7620\n",
            "Epoch 8/20\n",
            "29/29 [==============================] - 20s 679ms/step - loss: 0.2150 - accuracy: 0.9153 - val_loss: 0.2903 - val_accuracy: 0.8658\n",
            "Epoch 9/20\n",
            "29/29 [==============================] - 20s 696ms/step - loss: 0.2156 - accuracy: 0.9243 - val_loss: 0.2525 - val_accuracy: 0.8987\n",
            "Epoch 10/20\n",
            "29/29 [==============================] - 19s 666ms/step - loss: 0.1902 - accuracy: 0.9369 - val_loss: 0.1959 - val_accuracy: 0.9392\n",
            "Epoch 11/20\n",
            "29/29 [==============================] - 19s 672ms/step - loss: 0.1927 - accuracy: 0.9248 - val_loss: 0.1895 - val_accuracy: 0.9241\n",
            "Epoch 12/20\n",
            "29/29 [==============================] - 19s 666ms/step - loss: 0.1536 - accuracy: 0.9392 - val_loss: 0.1869 - val_accuracy: 0.9215\n",
            "Epoch 13/20\n",
            "29/29 [==============================] - 20s 675ms/step - loss: 0.1105 - accuracy: 0.9536 - val_loss: 0.1918 - val_accuracy: 0.9392\n",
            "Epoch 14/20\n",
            "29/29 [==============================] - 20s 701ms/step - loss: 0.0816 - accuracy: 0.9720 - val_loss: 0.1785 - val_accuracy: 0.9443\n",
            "Epoch 15/20\n",
            "29/29 [==============================] - 20s 696ms/step - loss: 0.0674 - accuracy: 0.9793 - val_loss: 0.1989 - val_accuracy: 0.9418\n",
            "Epoch 16/20\n",
            "29/29 [==============================] - 19s 671ms/step - loss: 0.0846 - accuracy: 0.9698 - val_loss: 0.1711 - val_accuracy: 0.9392\n",
            "Epoch 17/20\n",
            "29/29 [==============================] - 19s 667ms/step - loss: 0.0807 - accuracy: 0.9746 - val_loss: 0.2013 - val_accuracy: 0.9342\n",
            "Epoch 18/20\n",
            "29/29 [==============================] - 20s 682ms/step - loss: 0.0421 - accuracy: 0.9878 - val_loss: 0.2583 - val_accuracy: 0.9190\n",
            "Epoch 19/20\n",
            "29/29 [==============================] - 19s 668ms/step - loss: 0.0563 - accuracy: 0.9812 - val_loss: 0.2270 - val_accuracy: 0.9316\n",
            "Epoch 20/20\n",
            "29/29 [==============================] - 19s 674ms/step - loss: 0.0569 - accuracy: 0.9784 - val_loss: 0.2320 - val_accuracy: 0.9266\n",
            "INFO:tensorflow:Assets written to: face_detect/Models/{NAME}.model\\assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoAavghgxvyE"
      },
      "source": [
        "prepare() function prepares a test image according to the NN architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIft5X7dvpdW"
      },
      "source": [
        "def prepare(filepath):\r\n",
        "    img_size = 100 \r\n",
        "    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)/255  \r\n",
        "    img_resize = cv2.resize(img, (img_size, img_size))  \r\n",
        "    return img_resize.reshape(-1, img_size, img_size, 1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMN6xSfKxrRA"
      },
      "source": [
        "The numpy.argmax() function returns index of the max element of the array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52LgUP_5vwOT",
        "outputId": "9c535d0d-e7d8-411b-afe1-94519b89ba66"
      },
      "source": [
        "prediction = model.predict(prepare(\"Face_Mask/Test/IMG_1343.jpg\"))\n",
        "print((prediction))\n",
        "\n",
        "CATEGORIES = [\"Without_mask\" , \"With_mask\"]\n",
        "\n",
        "pred_class = CATEGORIES[np.argmax(prediction)]\n",
        "print(pred_class)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.94542986 0.05457013]]\nWithout_mask\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}